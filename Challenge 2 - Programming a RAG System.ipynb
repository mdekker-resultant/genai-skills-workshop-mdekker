{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "project_id = \"qwiklabs-gcp-00-711e7fbdb4c8\"\n",
        "dataset_id = \"AuroraFAQs\"\n",
        "location = \"US\"\n",
        "\n",
        "# --- INITIALIZE CLIENT ---\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# --- DEFINE DATASET REFERENCE ---\n",
        "dataset_ref = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
        "dataset_ref.location = location\n",
        "\n",
        "# --- CREATE DATASET IF NOT EXISTS ---\n",
        "try:\n",
        "    client.get_dataset(dataset_ref)\n",
        "    print(f\"Dataset '{dataset_id}' already exists.\")\n",
        "except Exception:\n",
        "    client.create_dataset(dataset_ref)\n",
        "    print(f\"Dataset '{dataset_id}' created in location '{location}'.\")"
      ],
      "metadata": {
        "id": "L9VbjIG7I2LQ"
      },
      "id": "L9VbjIG7I2LQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CREATE TABLE ---\n",
        "sql_create = \"\"\"\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS AuroraFAQs.aurora_faqs (\n",
        "    question STRING,\n",
        "    answer STRING\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# --- RUN CREATE TABLE JOB ---\n",
        "query_job = client.query(sql_create)"
      ],
      "metadata": {
        "id": "UgI1k1-rI1_y"
      },
      "id": "UgI1k1-rI1_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LOAD FILE ---\n",
        "sql_load = \"\"\"\n",
        "LOAD DATA OVERWRITE AuroraFAQs.aurora_faqs\n",
        "(question STRING,\n",
        "    answer STRING)\n",
        "FROM FILES (\n",
        "    format = 'CSV',\n",
        "    skip_leading_rows=1,\n",
        "    uris = ['gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv']\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# --- RUN LOAD FILE JOB ---\n",
        "query_job = client.query(sql_load)"
      ],
      "metadata": {
        "id": "bDBM0uNxI2Ne"
      },
      "id": "bDBM0uNxI2Ne",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    Set up external Connection us.embedding_conn connecting to Vertex AI: BigQuery Federation Connection Type\n",
        "      Vertex AI remote models, remote functions and BigLake (Cloud Resource)\n",
        "    Provided needed permissions to Service Account id via IAM tab\n",
        "    All done via GCP Interface\n",
        "'''"
      ],
      "metadata": {
        "id": "Sc67GDclavs_"
      },
      "id": "Sc67GDclavs_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LOAD MODEL ---\n",
        "sql_load_embeddings = \"\"\"\n",
        "CREATE OR REPLACE MODEL AuroraFAQs.Embeddings\n",
        "REMOTE WITH CONNECTION `us.embedding_conn`\n",
        "OPTIONS (ENDPOINT = 'text-embedding-005');\n",
        "\"\"\"\n",
        "\n",
        "# --- RUN LOAD MODEL JOB ---\n",
        "query_job = client.query(sql_load_embeddings)"
      ],
      "metadata": {
        "id": "gxUZvve8Rz9q"
      },
      "id": "gxUZvve8Rz9q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CREATE EMBEDDINGS ---\n",
        "sql_create_embeddings = \"\"\"\n",
        "CREATE OR REPLACE TABLE AuroraFAQs.aurora_faqs_embedded AS\n",
        "SELECT *\n",
        "FROM ML.GENERATE_EMBEDDING(\n",
        "    MODEL `AuroraFAQs.Embeddings`,\n",
        "    (SELECT CONCAT(question, ': ', answer) AS content FROM `AuroraFAQs.aurora_faqs`)\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# --- RUN CREATE EMBEDDINGS JOB ---\n",
        "query_job = client.query(sql_create_embeddings)\n"
      ],
      "metadata": {
        "id": "FkHjHeBgR557"
      },
      "id": "FkHjHeBgR557",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- QUERY ---\n",
        "question = \"local\" # Input question to be searched against the vector results\n",
        "\n",
        "sql_query_embeddings = \"\"\"\n",
        "CREATE OR REPLACE TABLE `AuroraFAQs.vector_search_result` AS\n",
        "SELECT\n",
        "    query.query,\n",
        "    base.content\n",
        "FROM\n",
        "    VECTOR_SEARCH(\n",
        "        TABLE `AuroraFAQs.aurora_faqs_embedded`,\n",
        "        'ml_generate_embedding_result',\n",
        "        (\n",
        "            SELECT\n",
        "                ml_generate_embedding_result,\n",
        "                content AS query\n",
        "            FROM\n",
        "                ML.GENERATE_EMBEDDING(\n",
        "                    MODEL `AuroraFAQs.Embeddings`,\n",
        "                    (SELECT '\"\"\" + question + \"\"\"' AS content)\n",
        "                )\n",
        "        ),\n",
        "        top_k => 5,\n",
        "        options => '{\"fraction_lists_to_search\": 0.01}'\n",
        "    );\n",
        "\"\"\"\n",
        "\n",
        "# --- RUN QUERY JOB ---\n",
        "query_job = client.query(sql_query_embeddings)\n"
      ],
      "metadata": {
        "id": "6-gS6yGBdR_b"
      },
      "id": "6-gS6yGBdR_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LOAD GEMINI ---\n",
        "sql_load_gemini = \"\"\"\n",
        "CREATE OR REPLACE MODEL AuroraFAQs.Gemini\n",
        "REMOTE WITH CONNECTION `us.embedding_conn`\n",
        "OPTIONS (ENDPOINT = 'gemini-2.0-flash');\n",
        "\"\"\"\n",
        "\n",
        "# --- RUN LOAD MODEL JOB ---\n",
        "query_job = client.query(sql_load_gemini)"
      ],
      "metadata": {
        "id": "YxYk8E48l6D8"
      },
      "id": "YxYk8E48l6D8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CREATE QUERY FUNCTION ---\n",
        "\n",
        "def search_similar_items(question: str):\n",
        "    # Safely escape single quotes\n",
        "    question_safe = question.replace(\"'\", \"\\\\'\")\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "      result.base.content AS content,\n",
        "      result.query.content AS query,\n",
        "      result.distance AS distance\n",
        "    FROM\n",
        "      VECTOR_SEARCH(\n",
        "        TABLE `AuroraFAQs.aurora_faqs_embedded`,\n",
        "        'ml_generate_embedding_result',\n",
        "        (\n",
        "          SELECT\n",
        "            content,\n",
        "            ml_generate_embedding_result\n",
        "          FROM\n",
        "            ML.GENERATE_EMBEDDING(\n",
        "              MODEL `AuroraFAQs.Embeddings`,\n",
        "              (SELECT '{question_safe}' AS content)\n",
        "            )\n",
        "        )\n",
        "        , top_k => 5,\n",
        "        options => '{{\"fraction_lists_to_search\": 0.01}}'\n",
        "      ) AS result\n",
        "    \"\"\"\n",
        "\n",
        "    query_job = client.query(query)\n",
        "    results = query_job.result()\n",
        "\n",
        "    # Return content, original query, and similarity distance\n",
        "    return [(row.content, row.query, row.distance) for row in results]\n",
        "\n",
        "\n",
        "# --- HAVE GEMINI SUMMARIZE ---\n",
        "\n",
        "def generate_answer_with_gemini(user_question: str):\n",
        "    # Step 1: Run vector search\n",
        "    top_matches = search_similar_items(user_question)\n",
        "\n",
        "    if not top_matches:\n",
        "        return \"Sorry, I couldn't find any relevant information.\"\n",
        "\n",
        "    # Step 2: Build prompt\n",
        "    reviews_text = \",\\n\".join([f\"review text: {text}\" for text in top_matches])\n",
        "    full_prompt = f\"Summarize retrieved FAQ results from customer question: {user_question}\\n{reviews_text}\"\n",
        "\n",
        "    # Step 3: Prepare safe parameterized query\n",
        "    query = \"\"\"\n",
        "    SELECT\n",
        "        ml_generate_text_llm_result AS generated\n",
        "    FROM\n",
        "        ML.GENERATE_TEXT(\n",
        "            MODEL `AuroraFAQs.Gemini`,\n",
        "            (\n",
        "                SELECT @prompt AS prompt\n",
        "            ),\n",
        "            STRUCT(\n",
        "                0.4 AS temperature,\n",
        "                300 AS max_output_tokens,\n",
        "                0.5 AS top_p,\n",
        "                5 AS top_k,\n",
        "                TRUE AS flatten_json_output\n",
        "            )\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    job_config = bigquery.QueryJobConfig(\n",
        "        query_parameters=[\n",
        "            bigquery.ScalarQueryParameter(\"prompt\", \"STRING\", full_prompt)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    results = client.query(query, job_config=job_config).result()\n",
        "\n",
        "    for row in results:\n",
        "        return row.generated\n",
        "\n",
        "\n",
        "# ---- CHAT LOOP ----\n",
        "def chat():\n",
        "    print(\"Please ask me a question (Type 'exit' to quit)\")\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() in (\"exit\", \"quit\"):\n",
        "            print(\"Goodbye\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Generate Gemini answer using top matching FAQ content\n",
        "            response = generate_answer_with_gemini(user_input)\n",
        "            print(f\"\\nGemini: {response}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# ---- RUN CHAT ----\n",
        "if __name__ == \"__main__\":\n",
        "    chat()"
      ],
      "metadata": {
        "id": "3Y6xBwHogSmz"
      },
      "id": "3Y6xBwHogSmz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Challenge 2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}